{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "zWzj1-XWrnE0",
   "metadata": {
    "id": "zWzj1-XWrnE0"
   },
   "source": [
    "# 0. 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30dc2cf-6fa1-4a29-80ca-96c189b13de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.23.4)\n",
      "Requirement already satisfied: catboost in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.23.4)\n",
      "Requirement already satisfied: matplotlib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (3.6.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.8.1)\n",
      "Requirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: plotly in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (5.11.0)\n",
      "Requirement already satisfied: graphviz in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (9.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib->catboost) (1.0.6)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from plotly->catboost) (8.1.0)\n",
      "Requirement already satisfied: lightgbm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (3.3.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (1.1.3)\n",
      "Requirement already satisfied: scipy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (1.8.1)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from lightgbm) (1.23.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: category_encoders in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.5.1.post0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from category_encoders) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from category_encoders) (1.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from category_encoders) (0.13.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from category_encoders) (1.1.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from category_encoders) (1.8.1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2022.6)\n",
      "Requirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from statsmodels>=0.9.0->category_encoders) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category_encoders) (3.0.9)\n",
      "Requirement already satisfied: optuna in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (3.0.3)\n",
      "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (4.11.4)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (1.4.44)\n",
      "Requirement already satisfied: colorlog in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: PyYAML in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (1.23.4)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (1.8.1)\n",
      "Requirement already satisfied: cliff in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (4.1.0)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from optuna) (0.9.0)\n",
      "Requirement already satisfied: Mako in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-metadata<5.0.0->optuna) (3.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cliff->optuna) (4.1.1)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cliff->optuna) (2.4.2)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cliff->optuna) (3.5.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from stevedore>=2.0.1->cliff->optuna) (5.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Requirement already satisfied: seaborn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.12.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (1.5.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (3.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (1.23.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Requirement already satisfied: matplot in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.1.9)\n",
      "Requirement already satisfied: pyloco>=0.0.134 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplot) (0.0.139)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplot) (3.6.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (21.3)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=3.1.1->matplot) (1.0.6)\n",
      "Requirement already satisfied: websocket-client in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyloco>=0.0.134->matplot) (1.4.2)\n",
      "Requirement already satisfied: typing in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyloco>=0.0.134->matplot) (3.7.4.3)\n",
      "Requirement already satisfied: SimpleWebSocketServer in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyloco>=0.0.134->matplot) (0.1.2)\n",
      "Requirement already satisfied: ushlex in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyloco>=0.0.134->matplot) (0.99.1)\n",
      "Requirement already satisfied: twine in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pyloco>=0.0.134->matplot) (4.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.1.1->matplot) (1.16.0)\n",
      "Requirement already satisfied: rich>=12.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (12.6.0)\n",
      "Requirement already satisfied: pkginfo>=1.8.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (1.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (4.11.4)\n",
      "Requirement already satisfied: requests>=2.20 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (2.28.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (1.26.12)\n",
      "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (0.10.1)\n",
      "Requirement already satisfied: keyring>=15.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (23.11.0)\n",
      "Requirement already satisfied: rfc3986>=1.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (2.0.0)\n",
      "Requirement already satisfied: readme-renderer>=35.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from twine->pyloco>=0.0.134->matplot) (37.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-metadata>=3.6->twine->pyloco>=0.0.134->matplot) (3.9.0)\n",
      "Requirement already satisfied: jaraco.classes in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from keyring>=15.1->twine->pyloco>=0.0.134->matplot) (3.2.3)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from keyring>=15.1->twine->pyloco>=0.0.134->matplot) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from keyring>=15.1->twine->pyloco>=0.0.134->matplot) (0.8.0)\n",
      "Requirement already satisfied: docutils>=0.13.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot) (0.19)\n",
      "Requirement already satisfied: bleach>=2.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot) (5.0.1)\n",
      "Requirement already satisfied: Pygments>=2.5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot) (2.13.0)\n",
      "Requirement already satisfied: webencodings in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from bleach>=2.1.0->readme-renderer>=35.0->twine->pyloco>=0.0.134->matplot) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->twine->pyloco>=0.0.134->matplot) (3.4)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from rich>=12.0.0->twine->pyloco>=0.0.134->matplot) (0.9.1)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from SecretStorage>=3.2->keyring>=15.1->twine->pyloco>=0.0.134->matplot) (38.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine->pyloco>=0.0.134->matplot) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.1->twine->pyloco>=0.0.134->matplot) (2.21)\n",
      "Requirement already satisfied: more-itertools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jaraco.classes->keyring>=15.1->twine->pyloco>=0.0.134->matplot) (9.0.0)\n",
      "Requirement already satisfied: imblearn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from imblearn) (0.9.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: pacmap in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.6.5)\n",
      "Requirement already satisfied: numba>=0.50 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pacmap) (0.56.4)\n",
      "Requirement already satisfied: annoy>=1.11 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pacmap) (1.17.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pacmap) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pacmap) (1.1.3)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from numba>=0.50->pacmap) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from numba>=0.50->pacmap) (65.5.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.20->pacmap) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.20->pacmap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.20->pacmap) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# 패키지 불러오기 -> 한 번만 하면 아마 뒤부터는 안해도 될 듯\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install catboost\n",
    "!pip install lightgbm\n",
    "!pip install category_encoders\n",
    "!pip install optuna\n",
    "!pip install seaborn\n",
    "!pip install matplot\n",
    "!pip install imblearn\n",
    "!pip install pacmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ESfNDxF0ro0Z",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1668066865801,
     "user": {
      "displayName": "신웅재",
      "userId": "10740416681947312578"
     },
     "user_tz": -540
    },
    "id": "ESfNDxF0ro0Z"
   },
   "outputs": [],
   "source": [
    "# 패키지 import 시키기 이건 해줘야 함\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.ensemble import * # ExtraTreesRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold # KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, f1_score, confusion_matrix \n",
    "# make_scorer : MSE 대신 사용자가 정의한 손실함수를 사용하고 싶을 때\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # LabelEncoder, StandardScaler\n",
    "# from sklearn.svm import * # SVC, SVR\n",
    "# from sklearn.inspection import *\n",
    "# from sklearn.linear_model import * # LogisticRegression, RANSACRegressor, Ridge, Lasso, ElasticNet\n",
    "# from sklearn.decomposition import * # PCA\n",
    "# from sklearn.cluster import *\n",
    "# import sklearn.neighbors._base\n",
    "\n",
    "# from category_encoders.ordinal import OrdinalEncoder\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "# from sklearn.cluster import KMeansuc\n",
    "# from kmodes.kmodes import KModes\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from FRUFS import FRUFS\n",
    "import pacmap\n",
    "# from missingpy import MissForest\n",
    "\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore') # 경고메세지를 무시하거나 숨긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "34b28ab3-0db8-475c-aa7e-384906e9b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Construction Machine Oil'  'Getting Started.ipynb'   images\n"
     ]
    }
   ],
   "source": [
    "# 경로 확인\n",
    "!ls '/home/studio-lab-user/MYDATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abf2835e-429d-4cc8-bb31-dcac16015d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_info.xlsx\tsample_submission.csv  test.csv  train.csv\n"
     ]
    }
   ],
   "source": [
    "# 경로 확인2\n",
    "!ls '/home/studio-lab-user/MYDATA/Construction Machine Oil/open/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "950cfd3e-e36f-42f6-a0cf-341b57022149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 54)\n",
      "761130\n",
      "(6041, 19)\n",
      "114779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMPONENT_ARBITRARY</th>\n",
       "      <th>ANONYMOUS_1</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>SAMPLE_TRANSFER_DAY</th>\n",
       "      <th>ANONYMOUS_2</th>\n",
       "      <th>AG</th>\n",
       "      <th>AL</th>\n",
       "      <th>B</th>\n",
       "      <th>BA</th>\n",
       "      <th>...</th>\n",
       "      <th>U25</th>\n",
       "      <th>U20</th>\n",
       "      <th>U14</th>\n",
       "      <th>U6</th>\n",
       "      <th>U4</th>\n",
       "      <th>V</th>\n",
       "      <th>V100</th>\n",
       "      <th>V40</th>\n",
       "      <th>ZN</th>\n",
       "      <th>Y_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>COMPONENT3</td>\n",
       "      <td>1486</td>\n",
       "      <td>2011</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.0</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>COMPONENT2</td>\n",
       "      <td>1350</td>\n",
       "      <td>2021</td>\n",
       "      <td>51</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>COMPONENT2</td>\n",
       "      <td>2415</td>\n",
       "      <td>2015</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11261.0</td>\n",
       "      <td>41081.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.6</td>\n",
       "      <td>412</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>COMPONENT3</td>\n",
       "      <td>7389</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>COMPONENT3</td>\n",
       "      <td>3954</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.1</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID COMPONENT_ARBITRARY  ANONYMOUS_1  YEAR  SAMPLE_TRANSFER_DAY  \\\n",
       "0  TRAIN_00000          COMPONENT3         1486  2011                    7   \n",
       "1  TRAIN_00001          COMPONENT2         1350  2021                   51   \n",
       "2  TRAIN_00002          COMPONENT2         2415  2015                    2   \n",
       "3  TRAIN_00003          COMPONENT3         7389  2010                    2   \n",
       "4  TRAIN_00004          COMPONENT3         3954  2015                    4   \n",
       "\n",
       "   ANONYMOUS_2  AG   AL    B  BA  ...  U25  U20   U14       U6       U4  V  \\\n",
       "0          200   0    3   93   0  ...  NaN  NaN   NaN      NaN      NaN  0   \n",
       "1          375   0    2   19   0  ...  2.0  4.0   6.0    216.0   1454.0  0   \n",
       "2          200   0  110    1   1  ...  0.0  3.0  39.0  11261.0  41081.0  0   \n",
       "3          200   0    8    3   0  ...  NaN  NaN   NaN      NaN      NaN  0   \n",
       "4          200   0    1  157   0  ...  NaN  NaN   NaN      NaN      NaN  0   \n",
       "\n",
       "   V100    V40   ZN  Y_LABEL  \n",
       "0   NaN  154.0   75        0  \n",
       "1   NaN   44.0  652        0  \n",
       "2   NaN   72.6  412        1  \n",
       "3   NaN  133.3    7        0  \n",
       "4   NaN  133.1  128        0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인한 경로대로 train 데이터와 test 데이터 불러오기\n",
    "path = '/home/studio-lab-user/MYDATA/Construction Machine Oil/open/'\n",
    "\n",
    "Rdata_train = pd.read_csv(path + 'train.csv')\n",
    "Rdata_test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "print(Rdata_train.shape) # (14095, 54) -> 생각보다 데이터 수가 적다.\n",
    "print(Rdata_train.size)\n",
    "print(Rdata_test.shape) # (6041, 19)\n",
    "print(Rdata_test.size)\n",
    "Rdata_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36f1b26e-367f-4754-813e-08adef014849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 copy 작업\n",
    "train1 = Rdata_train.copy()\n",
    "test1 = Rdata_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30d8b744-fd37-4e8f-94cd-9f1ff4fa6d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 11)\n",
      "(6041, 8)\n"
     ]
    }
   ],
   "source": [
    "# 변수 정하기\n",
    "# 아마 작업한다면 여기를 신경써야 할 것이다.\n",
    "# 어떤 변수를 빼고 넣어야 할지 다양하게 시도해 볼 것\n",
    "\n",
    "train2 = train1.loc[:, ['AL', 'BA', 'Y_LABEL',\n",
    "                        'ANONYMOUS_1', 'NI', 'CR', 'MN', 'FE', 'CU', 'PQINDEX', 'YEAR']]\n",
    "test2 = test1.loc[:, ['ANONYMOUS_1', 'NI', 'CR', 'MN', 'FE', 'CU', 'PQINDEX', 'YEAR']] # test 데이터에 ID 변수는 빼줘야 함\n",
    "\n",
    "print(train2.shape)\n",
    "print(test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aa897bf5-2d5b-44a1-b1dc-66ea6974a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 변수를 숫자로 바꾸기\n",
    "# 이로써 COMPONENT_ARBITRARY -> COMPONENT_ARBITRARY_category로 이름도 바뀌고 데이터도 바뀜\n",
    "# YEAR -> YEAR_category로 바뀜\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "\n",
    "# train2['COMPONENT_ARBITRARY_category'] = le1.fit_transform(train2['COMPONENT_ARBITRARY'])\n",
    "train2['YEAR_category'] = le2.fit_transform(train2['YEAR'])\n",
    "\n",
    "# test2['COMPONENT_ARBITRARY_category'] = le1.transform(test2['COMPONENT_ARBITRARY'])\n",
    "test2['YEAR_category'] = le2.transform(test2['YEAR'])\n",
    "\n",
    "train3 = train2.drop(['YEAR'], axis = 1)\n",
    "test3 = test2.drop(['YEAR'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "938ebd5f-14b2-41ab-9e9f-cb7b028dfef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 11)\n",
      "(6041, 8)\n"
     ]
    }
   ],
   "source": [
    "# CatBoost에서는 반드시 범주형 변수가 어떤 것인지를 지정해줘야함\n",
    "# 따라서 categorical_features 처럼 지정해줄 리스트가 필요\n",
    "# 사실 그 밑에 numeric 리스트 2개는 필요 없음\n",
    "\n",
    "categorical_features = ['YEAR_category']\n",
    "\n",
    "print(train3.shape)\n",
    "print(test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e55b2a35-d0c9-4f12-841f-c1eddbaf97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터에서 X_train은 Y_LABEL 빼고, y_train은 Y_LABEL만 넣고\n",
    "X_train = train3.drop(['Y_LABEL'], axis = 1)\n",
    "y_train = train3['Y_LABEL']\n",
    "X_test = test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1bba91f-80f9-4939-8e93-70f215f551ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9866, 10)\n",
      "(4229, 10)\n",
      "(9866,)\n",
      "(4229,)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN 데이터를 교차검증을 위해 PARTRAIN과 VAL로 나누기\n",
    "X_partrain, X_val, y_partrain, y_val = train_test_split(X_train, \n",
    "                                                        y_train, \n",
    "                                                        test_size = 0.3, \n",
    "                                                        random_state = 39, \n",
    "                                                        stratify = y_train)\n",
    "print(X_partrain.shape)\n",
    "print(X_val.shape)\n",
    "print(y_partrain.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a050483e-4ca3-4852-80ca-578029345e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTUNA를 위한 작업\n",
    "\n",
    "def objective(trial : Trial) -> float :\n",
    "\n",
    "    params_cat = {\n",
    "        \"random_state\" : 39,\n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 1), # -> 0.001에서 1 사이에 랜덤한 연속형 숫자를 지정해준다. learning_rate : 학습률\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100, 1000), # -> 100에서 1000까지 랜덤한 정수형 숫자를 지정해준다. n_estimators : 나무의 개수\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", 3, 10) # -> 3에서 10까지 랜덤한 정수형 숫자를 지정해준다. max_depth : 나뭇가지의 개수 \n",
    "  }\n",
    "    \n",
    "    model = CatBoostClassifier(**params_cat)\n",
    "    model.fit(X_partrain, y_partrain, eval_set = [(X_val, y_val)], # -> eval_set : 모델 적합은 partrain으로 하지만, 평가는 val로 한다.\n",
    "              early_stopping_rounds = 100, verbose = False, cat_features = categorical_features) # -> early_stopping_rounds : 100번까지도 성능이 안 좋아지면 멈추자.\n",
    "\n",
    "    cat_pred = model.predict(X_val)\n",
    "    AUC = roc_auc_score(y_val, cat_pred) # roc_auc_score로 두었지만, f1_score, accuracy_score 등 score는 다양하다.\n",
    "    \n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc6109a-d252-4cfe-85df-f93010fd8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-09 01:20:46,736]\u001b[0m A new study created in memory with name: cat_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:20:54,214]\u001b[0m Trial 0 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.04371872304807245, 'n_estimators': 818, 'max_depth': 9}. Best is trial 0 with value: 0.7667590743854684.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:04,517]\u001b[0m Trial 1 finished with value: 0.7570637835267428 and parameters: {'learning_rate': 0.002323537042351288, 'n_estimators': 642, 'max_depth': 7}. Best is trial 0 with value: 0.7667590743854684.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:13,714]\u001b[0m Trial 2 finished with value: 0.7678855843958668 and parameters: {'learning_rate': 0.024644795423723085, 'n_estimators': 524, 'max_depth': 8}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:17,326]\u001b[0m Trial 3 finished with value: 0.7578024962258691 and parameters: {'learning_rate': 0.5984000779343428, 'n_estimators': 834, 'max_depth': 10}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:20,355]\u001b[0m Trial 4 finished with value: 0.7604433135579381 and parameters: {'learning_rate': 0.5737932383737886, 'n_estimators': 473, 'max_depth': 9}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:21,868]\u001b[0m Trial 5 finished with value: 0.7604433135579381 and parameters: {'learning_rate': 0.68989080483343, 'n_estimators': 665, 'max_depth': 5}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:26,272]\u001b[0m Trial 6 finished with value: 0.7639889912829753 and parameters: {'learning_rate': 0.05984842261391979, 'n_estimators': 875, 'max_depth': 3}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:27,923]\u001b[0m Trial 7 finished with value: 0.7659834797629245 and parameters: {'learning_rate': 0.36677155109656834, 'n_estimators': 748, 'max_depth': 5}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:34,818]\u001b[0m Trial 8 finished with value: 0.7647277039821019 and parameters: {'learning_rate': 0.028590346350381008, 'n_estimators': 860, 'max_depth': 7}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:42,888]\u001b[0m Trial 9 finished with value: 0.7570637835267428 and parameters: {'learning_rate': 0.0029609935336908696, 'n_estimators': 574, 'max_depth': 4}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:47,556]\u001b[0m Trial 10 finished with value: 0.7623454181908809 and parameters: {'learning_rate': 0.020554880888828148, 'n_estimators': 208, 'max_depth': 8}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:21:54,058]\u001b[0m Trial 11 finished with value: 0.7636011939717033 and parameters: {'learning_rate': 0.08978169076864666, 'n_estimators': 398, 'max_depth': 10}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:18,957]\u001b[0m Trial 12 finished with value: 0.7662420113037723 and parameters: {'learning_rate': 0.009308627529030805, 'n_estimators': 1000, 'max_depth': 8}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:23,758]\u001b[0m Trial 13 finished with value: 0.7622161524204568 and parameters: {'learning_rate': 0.17701817230215108, 'n_estimators': 338, 'max_depth': 9}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:35,417]\u001b[0m Trial 14 finished with value: 0.7622161524204568 and parameters: {'learning_rate': 0.008710414004609248, 'n_estimators': 523, 'max_depth': 8}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:36,622]\u001b[0m Trial 15 finished with value: 0.7585780908484131 and parameters: {'learning_rate': 0.014346709630379867, 'n_estimators': 174, 'max_depth': 6}. Best is trial 2 with value: 0.7678855843958668.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:43,260]\u001b[0m Trial 16 finished with value: 0.7695291574879615 and parameters: {'learning_rate': 0.06550346273535652, 'n_estimators': 340, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:45,886]\u001b[0m Trial 17 finished with value: 0.7636011939717033 and parameters: {'learning_rate': 0.1380329171805048, 'n_estimators': 335, 'max_depth': 6}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:54,699]\u001b[0m Trial 18 finished with value: 0.758448825077989 and parameters: {'learning_rate': 0.006198623687001527, 'n_estimators': 271, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:22:58,816]\u001b[0m Trial 19 finished with value: 0.7665005428446204 and parameters: {'learning_rate': 0.1916990839832415, 'n_estimators': 443, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:03,676]\u001b[0m Trial 20 finished with value: 0.7647277039821019 and parameters: {'learning_rate': 0.06500815419343522, 'n_estimators': 583, 'max_depth': 7}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:12,696]\u001b[0m Trial 21 finished with value: 0.7648569697525259 and parameters: {'learning_rate': 0.038916052772456855, 'n_estimators': 967, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:34,432]\u001b[0m Trial 22 finished with value: 0.75580800774592 and parameters: {'learning_rate': 0.0011427886618785995, 'n_estimators': 742, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:36,929]\u001b[0m Trial 23 finished with value: 0.7612189081804822 and parameters: {'learning_rate': 0.037972317097749994, 'n_estimators': 135, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:45,417]\u001b[0m Trial 24 finished with value: 0.7652447670637978 and parameters: {'learning_rate': 0.018872398596033096, 'n_estimators': 356, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:51,414]\u001b[0m Trial 25 finished with value: 0.7649862355229499 and parameters: {'learning_rate': 0.10878676582435118, 'n_estimators': 498, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:57,010]\u001b[0m Trial 26 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.05143942612598516, 'n_estimators': 251, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:23:59,622]\u001b[0m Trial 27 finished with value: 0.7663712770741964 and parameters: {'learning_rate': 0.24385689898653637, 'n_estimators': 670, 'max_depth': 7}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:24:09,724]\u001b[0m Trial 28 finished with value: 0.7663712770741964 and parameters: {'learning_rate': 0.023477409385615884, 'n_estimators': 424, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:24:20,824]\u001b[0m Trial 29 finished with value: 0.7598338666292357 and parameters: {'learning_rate': 0.003262362133001468, 'n_estimators': 620, 'max_depth': 7}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:24:27,583]\u001b[0m Trial 30 finished with value: 0.7666298086150444 and parameters: {'learning_rate': 0.08138509310161161, 'n_estimators': 737, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:24:34,540]\u001b[0m Trial 31 finished with value: 0.7665005428446204 and parameters: {'learning_rate': 0.09640106174392557, 'n_estimators': 740, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:24:44,047]\u001b[0m Trial 32 finished with value: 0.7666298086150444 and parameters: {'learning_rate': 0.04385802308542244, 'n_estimators': 802, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:25:01,598]\u001b[0m Trial 33 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.013213984128673016, 'n_estimators': 931, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:25:24,195]\u001b[0m Trial 34 finished with value: 0.7652447670637978 and parameters: {'learning_rate': 0.008005114005418834, 'n_estimators': 892, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:25:45,651]\u001b[0m Trial 35 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.013213340415262786, 'n_estimators': 802, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:26:07,941]\u001b[0m Trial 36 finished with value: 0.7636011939717033 and parameters: {'learning_rate': 0.004951939122733445, 'n_estimators': 929, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:26:19,870]\u001b[0m Trial 37 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.027858828015617367, 'n_estimators': 686, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:26:38,233]\u001b[0m Trial 38 finished with value: 0.7648569697525259 and parameters: {'learning_rate': 0.013329728207831255, 'n_estimators': 812, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:26:40,455]\u001b[0m Trial 39 finished with value: 0.7632133966604314 and parameters: {'learning_rate': 0.3731164973806124, 'n_estimators': 626, 'max_depth': 6}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:26:47,631]\u001b[0m Trial 40 finished with value: 0.7648569697525259 and parameters: {'learning_rate': 0.06774436483122649, 'n_estimators': 539, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:27:09,144]\u001b[0m Trial 41 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.013710730151131244, 'n_estimators': 911, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:27:22,047]\u001b[0m Trial 42 finished with value: 0.7663712770741964 and parameters: {'learning_rate': 0.019606714958101786, 'n_estimators': 841, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:27:33,716]\u001b[0m Trial 43 finished with value: 0.7647277039821019 and parameters: {'learning_rate': 0.03263088111263976, 'n_estimators': 784, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:27:46,997]\u001b[0m Trial 44 finished with value: 0.7620868866500329 and parameters: {'learning_rate': 0.012403073303118115, 'n_estimators': 957, 'max_depth': 3}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:28:08,823]\u001b[0m Trial 45 finished with value: 0.7637304597421273 and parameters: {'learning_rate': 0.005474651147169448, 'n_estimators': 873, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:28:14,533]\u001b[0m Trial 46 finished with value: 0.7662420113037723 and parameters: {'learning_rate': 0.05123424281401507, 'n_estimators': 708, 'max_depth': 5}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:28:38,474]\u001b[0m Trial 47 finished with value: 0.7662420113037723 and parameters: {'learning_rate': 0.01043917166639351, 'n_estimators': 992, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:28:56,203]\u001b[0m Trial 48 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.023709456382110618, 'n_estimators': 466, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:29:10,691]\u001b[0m Trial 49 finished with value: 0.7653740328342218 and parameters: {'learning_rate': 0.01714635448869235, 'n_estimators': 849, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:29:23,083]\u001b[0m Trial 50 finished with value: 0.7610896424100582 and parameters: {'learning_rate': 0.0041266328661858295, 'n_estimators': 776, 'max_depth': 7}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:29:32,909]\u001b[0m Trial 51 finished with value: 0.7653740328342218 and parameters: {'learning_rate': 0.04189602606896424, 'n_estimators': 803, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:29:46,198]\u001b[0m Trial 52 finished with value: 0.7648569697525259 and parameters: {'learning_rate': 0.031147796764836182, 'n_estimators': 580, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:29:49,559]\u001b[0m Trial 53 finished with value: 0.7578948800728759 and parameters: {'learning_rate': 0.8488201056608734, 'n_estimators': 941, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:29:57,243]\u001b[0m Trial 54 finished with value: 0.7648569697525259 and parameters: {'learning_rate': 0.0752583824168358, 'n_estimators': 711, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:02,912]\u001b[0m Trial 55 finished with value: 0.7634719282012793 and parameters: {'learning_rate': 0.1258826459417342, 'n_estimators': 375, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:11,353]\u001b[0m Trial 56 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.04885989158225677, 'n_estimators': 269, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:16,863]\u001b[0m Trial 57 finished with value: 0.7598338666292357 and parameters: {'learning_rate': 0.006852879349911912, 'n_estimators': 296, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:20,040]\u001b[0m Trial 58 finished with value: 0.7662420113037723 and parameters: {'learning_rate': 0.050633741472957475, 'n_estimators': 197, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:21,550]\u001b[0m Trial 59 finished with value: 0.7571930492971667 and parameters: {'learning_rate': 0.01608643272629672, 'n_estimators': 141, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:28,042]\u001b[0m Trial 60 finished with value: 0.7626039497317288 and parameters: {'learning_rate': 0.010608861489029483, 'n_estimators': 260, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:37,677]\u001b[0m Trial 61 finished with value: 0.7652447670637978 and parameters: {'learning_rate': 0.043809673033682134, 'n_estimators': 319, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:47,325]\u001b[0m Trial 62 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.03368658709059278, 'n_estimators': 225, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:30:57,416]\u001b[0m Trial 63 finished with value: 0.7663712770741964 and parameters: {'learning_rate': 0.024230837868686645, 'n_estimators': 392, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:03,065]\u001b[0m Trial 64 finished with value: 0.7652447670637978 and parameters: {'learning_rate': 0.06546557931730497, 'n_estimators': 890, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:09,560]\u001b[0m Trial 65 finished with value: 0.7636011939717033 and parameters: {'learning_rate': 0.08385771456641172, 'n_estimators': 651, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:12,743]\u001b[0m Trial 66 finished with value: 0.7634719282012793 and parameters: {'learning_rate': 0.1396181480637843, 'n_estimators': 766, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:23,880]\u001b[0m Trial 67 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.027623619768925117, 'n_estimators': 828, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:36,789]\u001b[0m Trial 68 finished with value: 0.7634719282012793 and parameters: {'learning_rate': 0.02235762522339666, 'n_estimators': 826, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:46,380]\u001b[0m Trial 69 finished with value: 0.7666298086150444 and parameters: {'learning_rate': 0.02766036948156547, 'n_estimators': 501, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:31:55,082]\u001b[0m Trial 70 finished with value: 0.7598338666292357 and parameters: {'learning_rate': 0.007311700287568363, 'n_estimators': 291, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:05,933]\u001b[0m Trial 71 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.027486834077308, 'n_estimators': 437, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:09,242]\u001b[0m Trial 72 finished with value: 0.7638597255125513 and parameters: {'learning_rate': 0.0553300826935893, 'n_estimators': 106, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:17,046]\u001b[0m Trial 73 finished with value: 0.7637304597421273 and parameters: {'learning_rate': 0.03702720684120487, 'n_estimators': 499, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:23,313]\u001b[0m Trial 74 finished with value: 0.7665005428446204 and parameters: {'learning_rate': 0.10076875883751506, 'n_estimators': 916, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:34,334]\u001b[0m Trial 75 finished with value: 0.7619576208796088 and parameters: {'learning_rate': 0.018952681904286974, 'n_estimators': 558, 'max_depth': 7}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:43,647]\u001b[0m Trial 76 finished with value: 0.7663712770741964 and parameters: {'learning_rate': 0.045549303241139975, 'n_estimators': 866, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:51,598]\u001b[0m Trial 77 finished with value: 0.7665005428446204 and parameters: {'learning_rate': 0.02758194367136535, 'n_estimators': 602, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:32:59,884]\u001b[0m Trial 78 finished with value: 0.7624746839613048 and parameters: {'learning_rate': 0.011622385229458488, 'n_estimators': 495, 'max_depth': 6}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:33:09,790]\u001b[0m Trial 79 finished with value: 0.7638597255125513 and parameters: {'learning_rate': 0.009747373866022946, 'n_estimators': 350, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:33:28,073]\u001b[0m Trial 80 finished with value: 0.7663712770741964 and parameters: {'learning_rate': 0.015409364645485013, 'n_estimators': 719, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:33:36,677]\u001b[0m Trial 81 finished with value: 0.7678855843958668 and parameters: {'learning_rate': 0.0731293064555212, 'n_estimators': 747, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:33:43,973]\u001b[0m Trial 82 finished with value: 0.7651155012933739 and parameters: {'learning_rate': 0.06586492798970943, 'n_estimators': 762, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:33:53,014]\u001b[0m Trial 83 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.03722765667854363, 'n_estimators': 829, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:01,479]\u001b[0m Trial 84 finished with value: 0.7653740328342218 and parameters: {'learning_rate': 0.035787839507563216, 'n_estimators': 829, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:08,821]\u001b[0m Trial 85 finished with value: 0.7649862355229499 and parameters: {'learning_rate': 0.05734912109748028, 'n_estimators': 884, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:12,843]\u001b[0m Trial 86 finished with value: 0.7667590743854684 and parameters: {'learning_rate': 0.1899249016233077, 'n_estimators': 797, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:19,759]\u001b[0m Trial 87 finished with value: 0.7626039497317288 and parameters: {'learning_rate': 0.020336262874232965, 'n_estimators': 685, 'max_depth': 4}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:23,887]\u001b[0m Trial 88 finished with value: 0.7677563186254429 and parameters: {'learning_rate': 0.21062398812819955, 'n_estimators': 858, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:27,415]\u001b[0m Trial 89 finished with value: 0.7692706259471136 and parameters: {'learning_rate': 0.3564000561803845, 'n_estimators': 805, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:30,676]\u001b[0m Trial 90 finished with value: 0.7645984382116778 and parameters: {'learning_rate': 0.3239677853721607, 'n_estimators': 795, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:34,774]\u001b[0m Trial 91 finished with value: 0.7665005428446204 and parameters: {'learning_rate': 0.18369610186846289, 'n_estimators': 864, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:38,503]\u001b[0m Trial 92 finished with value: 0.7620868866500329 and parameters: {'learning_rate': 0.5265722093117092, 'n_estimators': 824, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:44,035]\u001b[0m Trial 93 finished with value: 0.7632133966604314 and parameters: {'learning_rate': 0.23214069320201963, 'n_estimators': 850, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:34:46,793]\u001b[0m Trial 94 finished with value: 0.7653371509108045 and parameters: {'learning_rate': 0.5836607385385385, 'n_estimators': 908, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:35:14,321]\u001b[0m Trial 95 finished with value: 0.758448825077989 and parameters: {'learning_rate': 0.0014623602215796275, 'n_estimators': 979, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:35:17,476]\u001b[0m Trial 96 finished with value: 0.7644691724412539 and parameters: {'learning_rate': 0.4504203235563009, 'n_estimators': 941, 'max_depth': 8}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:35:21,223]\u001b[0m Trial 97 finished with value: 0.7658542139925004 and parameters: {'learning_rate': 0.2955123739174765, 'n_estimators': 779, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:35:27,673]\u001b[0m Trial 98 finished with value: 0.7662420113037723 and parameters: {'learning_rate': 0.1475412472241857, 'n_estimators': 232, 'max_depth': 10}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 01:35:31,658]\u001b[0m Trial 99 finished with value: 0.7661127455333484 and parameters: {'learning_rate': 0.20971294840681715, 'n_estimators': 752, 'max_depth': 9}. Best is trial 16 with value: 0.7695291574879615.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed = 39)\n",
    "study = optuna.create_study(\n",
    "    study_name = \"cat_parameter_opt\",\n",
    "    direction = \"maximize\",   # 클수록 잘 맞춘다는 뜻이므로 maximize로 선택해준다.\n",
    "    sampler = sampler) \n",
    "study.optimize(objective, n_trials = 100) # 100번을 돌려서 가장 좋은 것을 찾아낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a434f19-8f30-4d3c-ae0e-dc2a3027272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.08653234054058978\n",
      "Best trial : {'learning_rate': 0.013666536260931852, 'n_estimators': 953, 'max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "# 가장 좋았던 score 값과 그 때의 초모수를 알려준다.\n",
    "print(\"Best Score :\", study.best_value)\n",
    "print(\"Best trial :\", study.best_trial.params)\n",
    "# 가장 좋았던 모델 'learning_rate': 0.03142344166841527, 'n_estimators': 513, 'max_depth': 6\n",
    "# 현재 모델 'learning_rate': 0.06550346273535652, 'n_estimators': 340, 'max_depth': 9 -> 0.7695291574879615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6f100ff1-2198-4b5d-a924-059d4a4598a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 2)\n",
      "(9866, 2)\n",
      "training model for CV #1\n",
      "training model for CV #2\n",
      "training model for CV #3\n",
      "training model for CV #4\n",
      "training model for CV #5\n"
     ]
    }
   ],
   "source": [
    "# 교차 검증을 통한 모델 적합\n",
    "# training model for CV #5 가 뜨면 끝난 것\n",
    "\n",
    "n_fold = 5 # 데이터를 5 등분해서 교차검증을 실시\n",
    "cv = StratifiedKFold(n_splits = n_fold, shuffle = True, random_state = 39) # 5등분 하되, y의 비율을 적절히 맞춰서 나눈다. -> y의 개수가 조금밖에 없으므로\n",
    " \n",
    "cat_val = np.zeros((X_train.shape[0], 2))\n",
    "cat_partrain = np.zeros((X_partrain.shape[0], 2))\n",
    "\n",
    "print(cat_val.shape)\n",
    "print(cat_partrain.shape)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(X_train, y_train), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    optuna_cat = CatBoostClassifier(\n",
    "        random_state = 39,\n",
    "        learning_rate = 0.06550346273535652,     # 초모수 꼭 바꿔줄 것\n",
    "        n_estimators = 340,                     # 초모수 꼭 바꿔줄 것\n",
    "        max_depth = 9)                          # 초모수 꼭 바꿔줄 것\n",
    "\n",
    "    optuna_cat.fit(X_train.loc[i_trn, :], y_train[i_trn], verbose = False)\n",
    "\n",
    "    cat_val[i_val, :] = optuna_cat.predict_proba(X_train.loc[i_val, :])\n",
    "    cat_partrain += optuna_cat.predict_proba(X_partrain) / n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5e032357-c395-4ebd-96d2-384b304ace31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>score</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.157275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12892</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.270325</td>\n",
       "      <td>7287</td>\n",
       "      <td>139</td>\n",
       "      <td>5605</td>\n",
       "      <td>1064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.365877</td>\n",
       "      <td>9852</td>\n",
       "      <td>253</td>\n",
       "      <td>3040</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.438913</td>\n",
       "      <td>11022</td>\n",
       "      <td>339</td>\n",
       "      <td>1870</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.493683</td>\n",
       "      <td>11651</td>\n",
       "      <td>402</td>\n",
       "      <td>1241</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.538787</td>\n",
       "      <td>12023</td>\n",
       "      <td>439</td>\n",
       "      <td>869</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.572965</td>\n",
       "      <td>12237</td>\n",
       "      <td>457</td>\n",
       "      <td>655</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.591526</td>\n",
       "      <td>12383</td>\n",
       "      <td>484</td>\n",
       "      <td>509</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.607266</td>\n",
       "      <td>12485</td>\n",
       "      <td>501</td>\n",
       "      <td>407</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.623598</td>\n",
       "      <td>12561</td>\n",
       "      <td>508</td>\n",
       "      <td>331</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.635566</td>\n",
       "      <td>12618</td>\n",
       "      <td>515</td>\n",
       "      <td>274</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.647283</td>\n",
       "      <td>12676</td>\n",
       "      <td>524</td>\n",
       "      <td>216</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.654634</td>\n",
       "      <td>12716</td>\n",
       "      <td>532</td>\n",
       "      <td>176</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.656790</td>\n",
       "      <td>12735</td>\n",
       "      <td>538</td>\n",
       "      <td>157</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.658304</td>\n",
       "      <td>12758</td>\n",
       "      <td>547</td>\n",
       "      <td>134</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.655821</td>\n",
       "      <td>12773</td>\n",
       "      <td>558</td>\n",
       "      <td>119</td>\n",
       "      <td>645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.654302</td>\n",
       "      <td>12789</td>\n",
       "      <td>568</td>\n",
       "      <td>103</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.650104</td>\n",
       "      <td>12791</td>\n",
       "      <td>575</td>\n",
       "      <td>101</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.653125</td>\n",
       "      <td>12802</td>\n",
       "      <td>576</td>\n",
       "      <td>90</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.652654</td>\n",
       "      <td>12813</td>\n",
       "      <td>582</td>\n",
       "      <td>79</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.656085</td>\n",
       "      <td>12825</td>\n",
       "      <td>583</td>\n",
       "      <td>67</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.653539</td>\n",
       "      <td>12830</td>\n",
       "      <td>589</td>\n",
       "      <td>62</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.649545</td>\n",
       "      <td>12833</td>\n",
       "      <td>596</td>\n",
       "      <td>59</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.650188</td>\n",
       "      <td>12839</td>\n",
       "      <td>598</td>\n",
       "      <td>53</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.648358</td>\n",
       "      <td>12840</td>\n",
       "      <td>601</td>\n",
       "      <td>52</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.647568</td>\n",
       "      <td>12844</td>\n",
       "      <td>604</td>\n",
       "      <td>48</td>\n",
       "      <td>599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.646388</td>\n",
       "      <td>12849</td>\n",
       "      <td>608</td>\n",
       "      <td>43</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.645969</td>\n",
       "      <td>12852</td>\n",
       "      <td>610</td>\n",
       "      <td>40</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.644809</td>\n",
       "      <td>12855</td>\n",
       "      <td>613</td>\n",
       "      <td>37</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.643288</td>\n",
       "      <td>12857</td>\n",
       "      <td>616</td>\n",
       "      <td>35</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>12860</td>\n",
       "      <td>618</td>\n",
       "      <td>32</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.641676</td>\n",
       "      <td>12863</td>\n",
       "      <td>621</td>\n",
       "      <td>29</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.641634</td>\n",
       "      <td>12865</td>\n",
       "      <td>622</td>\n",
       "      <td>27</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.640798</td>\n",
       "      <td>12869</td>\n",
       "      <td>625</td>\n",
       "      <td>23</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.640400</td>\n",
       "      <td>12870</td>\n",
       "      <td>626</td>\n",
       "      <td>22</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.640712</td>\n",
       "      <td>12873</td>\n",
       "      <td>627</td>\n",
       "      <td>19</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.641425</td>\n",
       "      <td>12875</td>\n",
       "      <td>627</td>\n",
       "      <td>17</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>12877</td>\n",
       "      <td>629</td>\n",
       "      <td>15</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.640983</td>\n",
       "      <td>12878</td>\n",
       "      <td>629</td>\n",
       "      <td>14</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>12879</td>\n",
       "      <td>633</td>\n",
       "      <td>13</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.635241</td>\n",
       "      <td>12879</td>\n",
       "      <td>637</td>\n",
       "      <td>13</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.633652</td>\n",
       "      <td>12881</td>\n",
       "      <td>640</td>\n",
       "      <td>11</td>\n",
       "      <td>563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.630570</td>\n",
       "      <td>12881</td>\n",
       "      <td>644</td>\n",
       "      <td>11</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.627051</td>\n",
       "      <td>12882</td>\n",
       "      <td>649</td>\n",
       "      <td>10</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.621591</td>\n",
       "      <td>12882</td>\n",
       "      <td>656</td>\n",
       "      <td>10</td>\n",
       "      <td>547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>12883</td>\n",
       "      <td>663</td>\n",
       "      <td>9</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>12886</td>\n",
       "      <td>670</td>\n",
       "      <td>6</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.603009</td>\n",
       "      <td>12888</td>\n",
       "      <td>682</td>\n",
       "      <td>4</td>\n",
       "      <td>521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.589474</td>\n",
       "      <td>12889</td>\n",
       "      <td>699</td>\n",
       "      <td>3</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.561865</td>\n",
       "      <td>12892</td>\n",
       "      <td>733</td>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold     score     TP   FP     FN    TN\n",
       "0        0.00  0.157275      0    0  12892  1203\n",
       "1        0.02  0.270325   7287  139   5605  1064\n",
       "2        0.04  0.365877   9852  253   3040   950\n",
       "3        0.06  0.438913  11022  339   1870   864\n",
       "4        0.08  0.493683  11651  402   1241   801\n",
       "5        0.10  0.538787  12023  439    869   764\n",
       "6        0.12  0.572965  12237  457    655   746\n",
       "7        0.14  0.591526  12383  484    509   719\n",
       "8        0.16  0.607266  12485  501    407   702\n",
       "9        0.18  0.623598  12561  508    331   695\n",
       "10       0.20  0.635566  12618  515    274   688\n",
       "11       0.22  0.647283  12676  524    216   679\n",
       "12       0.24  0.654634  12716  532    176   671\n",
       "13       0.26  0.656790  12735  538    157   665\n",
       "14       0.28  0.658304  12758  547    134   656\n",
       "15       0.30  0.655821  12773  558    119   645\n",
       "16       0.32  0.654302  12789  568    103   635\n",
       "17       0.34  0.650104  12791  575    101   628\n",
       "18       0.36  0.653125  12802  576     90   627\n",
       "19       0.38  0.652654  12813  582     79   621\n",
       "20       0.40  0.656085  12825  583     67   620\n",
       "21       0.42  0.653539  12830  589     62   614\n",
       "22       0.44  0.649545  12833  596     59   607\n",
       "23       0.46  0.650188  12839  598     53   605\n",
       "24       0.48  0.648358  12840  601     52   602\n",
       "25       0.50  0.647568  12844  604     48   599\n",
       "26       0.52  0.646388  12849  608     43   595\n",
       "27       0.54  0.645969  12852  610     40   593\n",
       "28       0.56  0.644809  12855  613     37   590\n",
       "29       0.58  0.643288  12857  616     35   587\n",
       "30       0.60  0.642857  12860  618     32   585\n",
       "31       0.62  0.641676  12863  621     29   582\n",
       "32       0.64  0.641634  12865  622     27   581\n",
       "33       0.66  0.640798  12869  625     23   578\n",
       "34       0.68  0.640400  12870  626     22   577\n",
       "35       0.70  0.640712  12873  627     19   576\n",
       "36       0.72  0.641425  12875  627     17   576\n",
       "37       0.74  0.640625  12877  629     15   574\n",
       "38       0.76  0.640983  12878  629     14   574\n",
       "39       0.78  0.638298  12879  633     13   570\n",
       "40       0.80  0.635241  12879  637     13   566\n",
       "41       0.82  0.633652  12881  640     11   563\n",
       "42       0.84  0.630570  12881  644     11   559\n",
       "43       0.86  0.627051  12882  649     10   554\n",
       "44       0.88  0.621591  12882  656     10   547\n",
       "45       0.90  0.616438  12883  663      9   540\n",
       "46       0.92  0.611940  12886  670      6   533\n",
       "47       0.94  0.603009  12888  682      4   521\n",
       "48       0.96  0.589474  12889  699      3   504\n",
       "49       0.98  0.561865  12892  733      0   470"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이건 그냥 참고용\n",
    "# 이걸 통해서 내 모형이 얼마나 맞췄는지를 그냥 볼 수 있다.\n",
    "# TP : 정상이라 예측했는데 실제로 정상인 것\n",
    "# FP : 정상이라 예측했는데 실제로 불량인 것\n",
    "# FN : 불량이라 예측했는데 실제로 정상인 것\n",
    "# TN : 불량이라 예측했는데 실제로 불량인 것\n",
    "# 아마 해보면 FP의 비율이 클 것이다. -> 정상 속에 숨어 있는 불량을 뽑아내야 한다.\n",
    "\n",
    "scores = []\n",
    "TP = []\n",
    "FP = []\n",
    "FN = []\n",
    "TN = []\n",
    "for threshold in range(50) :\n",
    "    threshold = threshold / 50\n",
    "    pred = cat_val[:, 1]\n",
    "    pred = np.where(pred >= threshold, 1, 0)\n",
    "    score = f1_score(y_train, pred)\n",
    "    scores.append(score)\n",
    "    TP.append(confusion_matrix(y_train, pred)[0][0])\n",
    "    FN.append(confusion_matrix(y_train, pred)[0][1])\n",
    "    FP.append(confusion_matrix(y_train, pred)[1][0])\n",
    "    TN.append(confusion_matrix(y_train, pred)[1][1])\n",
    "    \n",
    "\n",
    "temp1 = pd.DataFrame(np.linspace(0, 0.98, 50), columns = ['threshold'])\n",
    "temp2 = pd.DataFrame(scores, columns = ['score'])\n",
    "temp3 = pd.DataFrame(TP, columns = ['TP'])\n",
    "temp4 = pd.DataFrame(FP, columns = ['FP'])\n",
    "temp5 = pd.DataFrame(FN, columns = ['FN'])\n",
    "temp6 = pd.DataFrame(TN, columns = ['TN'])\n",
    "scores = pd.concat([temp1, temp2, temp3, temp4, temp5, temp6], axis = 1)\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cf0f916-3eb2-40af-9664-ff90239743f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 2)\n",
      "(9866, 2)\n",
      "(14095, 11)\n"
     ]
    }
   ],
   "source": [
    "# 예측한 불량률을 다시 TRAIN 데이터에 넣어준다.\n",
    "# 예측한 불량률이 STUDENT MODEL에서 Y가 될 것이다.\n",
    "\n",
    "print(cat_val.shape)\n",
    "print(cat_partrain.shape)\n",
    "\n",
    "X_train['model1_prob'] = cat_val[:, 1]\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "832f3c86-6aa1-47fa-aa67-fd0687b54f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 18)\n",
      "(14095,)\n",
      "(6041, 18)\n"
     ]
    }
   ],
   "source": [
    "X_train2 = train1.loc[:, ['COMPONENT_ARBITRARY', 'ANONYMOUS_1', 'YEAR', 'ANONYMOUS_2', 'AG',\n",
    "                                        'CO', 'CR', 'CU', 'FE', 'H2O', 'MN', 'MO', 'NI', 'PQINDEX', 'TI', 'V',\n",
    "                                        'V40', 'ZN']]\n",
    "y_train2 = cat_val[:, 1]\n",
    "print(X_train2.shape)\n",
    "print(y_train2.shape)\n",
    "\n",
    "X_test = test1.loc[:, ['COMPONENT_ARBITRARY', 'ANONYMOUS_1', 'YEAR', 'ANONYMOUS_2', 'AG',\n",
    "                                        'CO', 'CR', 'CU', 'FE', 'H2O', 'MN', 'MO', 'NI', 'PQINDEX', 'TI', 'V',\n",
    "                                        'V40', 'ZN']]\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9945760a-3457-4d59-a798-534c04e8a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 데이터에 없는 변수들을 빼줘야 한다.\n",
    "# X_train에서는 만약 AL, BA 말고도 새로운 변수들을 넣었다면 여기서 빼줘야 한다. 18개 변수 제외하고 말이다.\n",
    "# X_train에서는 꼭 model1_prob도 빼줘야 한다. -> y_train 이므로\n",
    "# X_train2 = X_train.loc[:, ['ANONYMOUS_1', 'NI', 'CR', 'MN', 'FE', 'CU', 'PQINDEX', 'YEAR_category']]\n",
    "# y_train2 = X_train['model1_prob']\n",
    "# print(X_train2.shape)\n",
    "# print(y_train2.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4fede426-9972-4f47-820e-6b097d5831e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 18)\n",
      "(6041, 18)\n"
     ]
    }
   ],
   "source": [
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "\n",
    "X_train2['COMPONENT_ARBITRARY_category'] = le1.fit_transform(X_train2['COMPONENT_ARBITRARY'])\n",
    "X_train2['YEAR_category'] = le2.fit_transform(X_train2['YEAR'])\n",
    "\n",
    "X_test['COMPONENT_ARBITRARY_category'] = le1.transform(X_test['COMPONENT_ARBITRARY'])\n",
    "X_test['YEAR_category'] = le2.transform(X_test['YEAR'])\n",
    "\n",
    "X_train2.drop(['COMPONENT_ARBITRARY'], axis = 1, inplace = True)\n",
    "X_test.drop(['COMPONENT_ARBITRARY'], axis = 1, inplace = True)\n",
    "\n",
    "X_train2.drop(['YEAR'], axis = 1, inplace = True)\n",
    "X_test.drop(['YEAR'], axis = 1, inplace = True)\n",
    "\n",
    "print(X_train2.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0a661b36-1d00-41b2-9727-fd76e39223b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 18)\n",
      "(6041, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "temp_train = X_train2.copy()\n",
    "temp_test = X_test.copy()\n",
    "X_train2 = sc.fit_transform(X_train2)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print(X_train2.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e2cba3c8-e44f-4d47-8e32-0fc8163a18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PM = pacmap.PaCMAP(n_components = 18, verbose = False, n_neighbors = None, MN_ratio = 0.5, FP_ratio = 2.0)\n",
    "X_train2 = PM.fit_transform(np.array(X_train2), init = \"pca\")\n",
    "X_test = PM.transform(np.array(X_test), init = \"pca\", basis = np.array(X_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e7ff66fa-af99-490a-8e98-4ce2fe962fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = pd.DataFrame(X_train2, index = temp_train.index, columns = temp_train.columns)\n",
    "X_test = pd.DataFrame(X_test, index = temp_test.index, columns = temp_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "gtPCJWa8qd16",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1668058269063,
     "user": {
      "displayName": "신웅재",
      "userId": "10740416681947312578"
     },
     "user_tz": -540
    },
    "id": "gtPCJWa8qd16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9866, 18)\n",
      "(4229, 18)\n",
      "(9866,)\n",
      "(4229,)\n"
     ]
    }
   ],
   "source": [
    "X_partrain, X_val, y_partrain, y_val = train_test_split(X_train2, y_train2, test_size = 0.3, random_state = 39)\n",
    "print(X_partrain.shape)\n",
    "print(X_val.shape)\n",
    "print(y_partrain.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5_iPm04Fqdze",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1668058269063,
     "user": {
      "displayName": "신웅재",
      "userId": "10740416681947312578"
     },
     "user_tz": -540
    },
    "id": "5_iPm04Fqdze"
   },
   "outputs": [],
   "source": [
    "def objective(trial : Trial) -> float :\n",
    "\n",
    "    params_cat = {\n",
    "        \"random_state\" : 39,\n",
    "        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.001, 1),\n",
    "        \"n_estimators\" : trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\" : trial.suggest_int(\"max_depth\", 3, 10)\n",
    "  }\n",
    "    \n",
    "    model = CatBoostRegressor(**params_cat)\n",
    "    model.fit(X_partrain, y_partrain, eval_set = [(X_val, y_val)],\n",
    "              early_stopping_rounds = 100, verbose = False)\n",
    "\n",
    "    cat_pred = model.predict(X_val)\n",
    "    MAE = mean_absolute_error(y_val, cat_pred) # 내가 사용한건 MAE 이지만, RMSE 등 다양한 회귀 지표 들이 있다.\n",
    "    \n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "WEJSsNsKqdxC",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668058269063,
     "user": {
      "displayName": "신웅재",
      "userId": "10740416681947312578"
     },
     "user_tz": -540
    },
    "id": "WEJSsNsKqdxC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-12-09 07:07:59,211]\u001b[0m A new study created in memory with name: cat_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:07,711]\u001b[0m Trial 0 finished with value: 0.09111468339751001 and parameters: {'learning_rate': 0.04371872304807245, 'n_estimators': 818, 'max_depth': 9}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:14,935]\u001b[0m Trial 1 finished with value: 0.09228462310325888 and parameters: {'learning_rate': 0.002323537042351288, 'n_estimators': 642, 'max_depth': 7}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:22,297]\u001b[0m Trial 2 finished with value: 0.09118712166772219 and parameters: {'learning_rate': 0.024644795423723085, 'n_estimators': 524, 'max_depth': 8}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:27,490]\u001b[0m Trial 3 finished with value: 0.09252094354638427 and parameters: {'learning_rate': 0.5984000779343428, 'n_estimators': 834, 'max_depth': 10}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:30,987]\u001b[0m Trial 4 finished with value: 0.09148674537338619 and parameters: {'learning_rate': 0.5737932383737886, 'n_estimators': 473, 'max_depth': 9}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:31,531]\u001b[0m Trial 5 finished with value: 0.09173955693572784 and parameters: {'learning_rate': 0.68989080483343, 'n_estimators': 665, 'max_depth': 5}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:32,424]\u001b[0m Trial 6 finished with value: 0.09189176211234922 and parameters: {'learning_rate': 0.05984842261391979, 'n_estimators': 875, 'max_depth': 3}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:33,081]\u001b[0m Trial 7 finished with value: 0.09162832910098166 and parameters: {'learning_rate': 0.36677155109656834, 'n_estimators': 748, 'max_depth': 5}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:36,961]\u001b[0m Trial 8 finished with value: 0.09129763109516628 and parameters: {'learning_rate': 0.028590346350381008, 'n_estimators': 860, 'max_depth': 7}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:39,312]\u001b[0m Trial 9 finished with value: 0.09282450053256222 and parameters: {'learning_rate': 0.0029609935336908696, 'n_estimators': 574, 'max_depth': 4}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:47,471]\u001b[0m Trial 10 finished with value: 0.09117299090420977 and parameters: {'learning_rate': 0.09900330892141676, 'n_estimators': 208, 'max_depth': 10}. Best is trial 0 with value: 0.09111468339751001.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:08:59,376]\u001b[0m Trial 11 finished with value: 0.0907901368940774 and parameters: {'learning_rate': 0.08978169076864666, 'n_estimators': 179, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:06,593]\u001b[0m Trial 12 finished with value: 0.09175233530316076 and parameters: {'learning_rate': 0.009462079419610504, 'n_estimators': 195, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:11,720]\u001b[0m Trial 13 finished with value: 0.09130930451613906 and parameters: {'learning_rate': 0.17701817230215108, 'n_estimators': 373, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:24,581]\u001b[0m Trial 14 finished with value: 0.09107071166222179 and parameters: {'learning_rate': 0.010402839097260867, 'n_estimators': 962, 'max_depth': 8}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:35,238]\u001b[0m Trial 15 finished with value: 0.09119276543795478 and parameters: {'learning_rate': 0.01131465881897402, 'n_estimators': 992, 'max_depth': 8}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:37,709]\u001b[0m Trial 16 finished with value: 0.0920902371568489 and parameters: {'learning_rate': 0.007918650666147933, 'n_estimators': 296, 'max_depth': 6}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:45,300]\u001b[0m Trial 17 finished with value: 0.09313308136145894 and parameters: {'learning_rate': 0.0011843637694741026, 'n_estimators': 394, 'max_depth': 8}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:50,899]\u001b[0m Trial 18 finished with value: 0.09158693570445366 and parameters: {'learning_rate': 0.1251966348401416, 'n_estimators': 115, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:09:54,743]\u001b[0m Trial 19 finished with value: 0.09123893515728214 and parameters: {'learning_rate': 0.01947610366156639, 'n_estimators': 999, 'max_depth': 6}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:02,672]\u001b[0m Trial 20 finished with value: 0.09181773321165813 and parameters: {'learning_rate': 0.00506921897643374, 'n_estimators': 404, 'max_depth': 8}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:09,082]\u001b[0m Trial 21 finished with value: 0.09127737274428342 and parameters: {'learning_rate': 0.057366641139189364, 'n_estimators': 757, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:19,293]\u001b[0m Trial 22 finished with value: 0.09093947266257477 and parameters: {'learning_rate': 0.06560691046581238, 'n_estimators': 948, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:25,859]\u001b[0m Trial 23 finished with value: 0.09157551034966954 and parameters: {'learning_rate': 0.21147871302172547, 'n_estimators': 936, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:34,454]\u001b[0m Trial 24 finished with value: 0.09083838510825594 and parameters: {'learning_rate': 0.0825854110114347, 'n_estimators': 930, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:44,106]\u001b[0m Trial 25 finished with value: 0.09082335179826044 and parameters: {'learning_rate': 0.08264663517695515, 'n_estimators': 721, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:50,765]\u001b[0m Trial 26 finished with value: 0.09140766267361149 and parameters: {'learning_rate': 0.29064500848991554, 'n_estimators': 684, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:10:55,411]\u001b[0m Trial 27 finished with value: 0.09141679498747003 and parameters: {'learning_rate': 0.12003903548400387, 'n_estimators': 755, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:11:05,256]\u001b[0m Trial 28 finished with value: 0.09101397273529975 and parameters: {'learning_rate': 0.09486051339138037, 'n_estimators': 619, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:11:16,009]\u001b[0m Trial 29 finished with value: 0.09106331518944909 and parameters: {'learning_rate': 0.03570441871623723, 'n_estimators': 784, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:11:21,640]\u001b[0m Trial 30 finished with value: 0.09106929635737286 and parameters: {'learning_rate': 0.019760649397088548, 'n_estimators': 896, 'max_depth': 7}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:11:31,995]\u001b[0m Trial 31 finished with value: 0.09093455547451937 and parameters: {'learning_rate': 0.04935101619944858, 'n_estimators': 919, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:11:48,432]\u001b[0m Trial 32 finished with value: 0.09106414314310443 and parameters: {'learning_rate': 0.04422414908011106, 'n_estimators': 913, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:11:53,465]\u001b[0m Trial 33 finished with value: 0.09171243593197612 and parameters: {'learning_rate': 0.17998471177238606, 'n_estimators': 714, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:06,056]\u001b[0m Trial 34 finished with value: 0.09087772506675079 and parameters: {'learning_rate': 0.08201177762073658, 'n_estimators': 816, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:13,191]\u001b[0m Trial 35 finished with value: 0.09135158533931148 and parameters: {'learning_rate': 0.07884905625381305, 'n_estimators': 818, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:18,893]\u001b[0m Trial 36 finished with value: 0.09141028070421156 and parameters: {'learning_rate': 0.2675533757787991, 'n_estimators': 601, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:22,339]\u001b[0m Trial 37 finished with value: 0.09145147531648169 and parameters: {'learning_rate': 0.4287883364802256, 'n_estimators': 536, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:29,917]\u001b[0m Trial 38 finished with value: 0.09096043483079412 and parameters: {'learning_rate': 0.12023725072280748, 'n_estimators': 819, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:35,534]\u001b[0m Trial 39 finished with value: 0.09115039689926044 and parameters: {'learning_rate': 0.034198090349639054, 'n_estimators': 698, 'max_depth': 8}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:41,654]\u001b[0m Trial 40 finished with value: 0.09306033442468477 and parameters: {'learning_rate': 0.8917097698490208, 'n_estimators': 467, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:12:51,876]\u001b[0m Trial 41 finished with value: 0.09106392857683672 and parameters: {'learning_rate': 0.05327474634726848, 'n_estimators': 857, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:05,793]\u001b[0m Trial 42 finished with value: 0.09122686174585946 and parameters: {'learning_rate': 0.02158598387486758, 'n_estimators': 891, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:06,602]\u001b[0m Trial 43 finished with value: 0.09195423809789147 and parameters: {'learning_rate': 0.07832930069691085, 'n_estimators': 812, 'max_depth': 3}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:21,172]\u001b[0m Trial 44 finished with value: 0.09119015639173003 and parameters: {'learning_rate': 0.039426676670332284, 'n_estimators': 654, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:21,809]\u001b[0m Trial 45 finished with value: 0.09145003663371595 and parameters: {'learning_rate': 0.14084145521908534, 'n_estimators': 847, 'max_depth': 4}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:33,823]\u001b[0m Trial 46 finished with value: 0.09116231434029567 and parameters: {'learning_rate': 0.026909279330069504, 'n_estimators': 726, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:42,120]\u001b[0m Trial 47 finished with value: 0.09082429934648643 and parameters: {'learning_rate': 0.06986132249430933, 'n_estimators': 791, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:49,936]\u001b[0m Trial 48 finished with value: 0.09083311436319895 and parameters: {'learning_rate': 0.0763708547775533, 'n_estimators': 497, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:53,737]\u001b[0m Trial 49 finished with value: 0.09157431070399989 and parameters: {'learning_rate': 0.16317312417078234, 'n_estimators': 481, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:13:55,063]\u001b[0m Trial 50 finished with value: 0.09149435301951941 and parameters: {'learning_rate': 0.24470240930991816, 'n_estimators': 122, 'max_depth': 7}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:05,571]\u001b[0m Trial 51 finished with value: 0.09094628586179862 and parameters: {'learning_rate': 0.08876461651427767, 'n_estimators': 329, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:17,236]\u001b[0m Trial 52 finished with value: 0.09098206620625901 and parameters: {'learning_rate': 0.06726728967358789, 'n_estimators': 270, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:25,097]\u001b[0m Trial 53 finished with value: 0.0912766143046947 and parameters: {'learning_rate': 0.11284315683653537, 'n_estimators': 568, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:34,036]\u001b[0m Trial 54 finished with value: 0.09107006191808821 and parameters: {'learning_rate': 0.07002466497535635, 'n_estimators': 783, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:42,136]\u001b[0m Trial 55 finished with value: 0.09103100896695955 and parameters: {'learning_rate': 0.090831130734909, 'n_estimators': 441, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:46,170]\u001b[0m Trial 56 finished with value: 0.09165936679802225 and parameters: {'learning_rate': 0.15960909017717106, 'n_estimators': 211, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:55,315]\u001b[0m Trial 57 finished with value: 0.09111084240854304 and parameters: {'learning_rate': 0.05383343085179565, 'n_estimators': 957, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:14:56,202]\u001b[0m Trial 58 finished with value: 0.09213888782512984 and parameters: {'learning_rate': 0.203029052722715, 'n_estimators': 782, 'max_depth': 5}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:15:04,067]\u001b[0m Trial 59 finished with value: 0.09111566918282799 and parameters: {'learning_rate': 0.029224183016864476, 'n_estimators': 880, 'max_depth': 8}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:15:21,061]\u001b[0m Trial 60 finished with value: 0.09105258340132875 and parameters: {'learning_rate': 0.015012672174283676, 'n_estimators': 674, 'max_depth': 9}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:15:30,377]\u001b[0m Trial 61 finished with value: 0.09128761970084316 and parameters: {'learning_rate': 0.04762727015154959, 'n_estimators': 914, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:15:42,684]\u001b[0m Trial 62 finished with value: 0.09114848180545691 and parameters: {'learning_rate': 0.042414003714574544, 'n_estimators': 971, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:15:50,894]\u001b[0m Trial 63 finished with value: 0.09112987956547268 and parameters: {'learning_rate': 0.05947946140281036, 'n_estimators': 931, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:15:59,357]\u001b[0m Trial 64 finished with value: 0.09127403854233639 and parameters: {'learning_rate': 0.10227704144473959, 'n_estimators': 866, 'max_depth': 10}. Best is trial 11 with value: 0.0907901368940774.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:09,324]\u001b[0m Trial 65 finished with value: 0.090763237024211 and parameters: {'learning_rate': 0.07499794344447426, 'n_estimators': 832, 'max_depth': 10}. Best is trial 65 with value: 0.090763237024211.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:16,176]\u001b[0m Trial 66 finished with value: 0.0910782954769904 and parameters: {'learning_rate': 0.1405020243605203, 'n_estimators': 734, 'max_depth': 10}. Best is trial 65 with value: 0.090763237024211.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:19,324]\u001b[0m Trial 67 finished with value: 0.09220858506233313 and parameters: {'learning_rate': 0.3542390316669989, 'n_estimators': 633, 'max_depth': 9}. Best is trial 65 with value: 0.090763237024211.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:28,563]\u001b[0m Trial 68 finished with value: 0.09064165837890989 and parameters: {'learning_rate': 0.07520569152907523, 'n_estimators': 795, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:37,008]\u001b[0m Trial 69 finished with value: 0.0912714789610842 and parameters: {'learning_rate': 0.06736382932101516, 'n_estimators': 763, 'max_depth': 9}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:46,625]\u001b[0m Trial 70 finished with value: 0.09107383264370576 and parameters: {'learning_rate': 0.03497909933777945, 'n_estimators': 147, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:16:59,625]\u001b[0m Trial 71 finished with value: 0.09078166297047098 and parameters: {'learning_rate': 0.07922949387415772, 'n_estimators': 810, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:17:08,319]\u001b[0m Trial 72 finished with value: 0.09100120761696415 and parameters: {'learning_rate': 0.10804039424625454, 'n_estimators': 842, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:17:15,557]\u001b[0m Trial 73 finished with value: 0.09105268883890387 and parameters: {'learning_rate': 0.14115003596574682, 'n_estimators': 783, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:17:26,360]\u001b[0m Trial 74 finished with value: 0.09084895162963909 and parameters: {'learning_rate': 0.07863755845969485, 'n_estimators': 830, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:17:33,333]\u001b[0m Trial 75 finished with value: 0.09124537055356799 and parameters: {'learning_rate': 0.06403662708091631, 'n_estimators': 703, 'max_depth': 9}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:17:40,933]\u001b[0m Trial 76 finished with value: 0.09131604734219569 and parameters: {'learning_rate': 0.09875174404519543, 'n_estimators': 795, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:18:20,115]\u001b[0m Trial 77 finished with value: 0.09224733943650024 and parameters: {'learning_rate': 0.0012527256875033619, 'n_estimators': 743, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:18:26,495]\u001b[0m Trial 78 finished with value: 0.0912510989554053 and parameters: {'learning_rate': 0.047056204625188704, 'n_estimators': 605, 'max_depth': 9}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:18:32,740]\u001b[0m Trial 79 finished with value: 0.09107396218447261 and parameters: {'learning_rate': 0.20367675915997055, 'n_estimators': 901, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:18:34,104]\u001b[0m Trial 80 finished with value: 0.09155176120060877 and parameters: {'learning_rate': 0.12466272799739855, 'n_estimators': 504, 'max_depth': 6}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:18:45,867]\u001b[0m Trial 81 finished with value: 0.09077447671686903 and parameters: {'learning_rate': 0.078833872939783, 'n_estimators': 824, 'max_depth': 10}. Best is trial 68 with value: 0.09064165837890989.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:18:55,083]\u001b[0m Trial 82 finished with value: 0.09058143935845717 and parameters: {'learning_rate': 0.07525919980319846, 'n_estimators': 805, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:19:06,595]\u001b[0m Trial 83 finished with value: 0.09089574628158614 and parameters: {'learning_rate': 0.05405784176393146, 'n_estimators': 762, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:19:17,470]\u001b[0m Trial 84 finished with value: 0.09102931495951869 and parameters: {'learning_rate': 0.07321665558804795, 'n_estimators': 803, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:19:26,774]\u001b[0m Trial 85 finished with value: 0.09110158595238305 and parameters: {'learning_rate': 0.09514638584720819, 'n_estimators': 868, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:19:35,152]\u001b[0m Trial 86 finished with value: 0.09118560704243821 and parameters: {'learning_rate': 0.041111538188286105, 'n_estimators': 853, 'max_depth': 9}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:19:48,374]\u001b[0m Trial 87 finished with value: 0.09089707764869864 and parameters: {'learning_rate': 0.05832716924020647, 'n_estimators': 832, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:19:55,122]\u001b[0m Trial 88 finished with value: 0.0909582980614275 and parameters: {'learning_rate': 0.12981275041713386, 'n_estimators': 338, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:02,032]\u001b[0m Trial 89 finished with value: 0.09114771974106375 and parameters: {'learning_rate': 0.08789553662065448, 'n_estimators': 728, 'max_depth': 9}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:03,169]\u001b[0m Trial 90 finished with value: 0.09195153257803045 and parameters: {'learning_rate': 0.023609161527009412, 'n_estimators': 254, 'max_depth': 4}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:13,638]\u001b[0m Trial 91 finished with value: 0.09078706822076187 and parameters: {'learning_rate': 0.07974645312383079, 'n_estimators': 771, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:22,096]\u001b[0m Trial 92 finished with value: 0.09118622482470172 and parameters: {'learning_rate': 0.10888240937158032, 'n_estimators': 764, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:30,774]\u001b[0m Trial 93 finished with value: 0.09074192118612297 and parameters: {'learning_rate': 0.0738664704294802, 'n_estimators': 688, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:36,832]\u001b[0m Trial 94 finished with value: 0.09127424878926346 and parameters: {'learning_rate': 0.16605511147660937, 'n_estimators': 653, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:20:48,672]\u001b[0m Trial 95 finished with value: 0.09075194748440545 and parameters: {'learning_rate': 0.05731748812976315, 'n_estimators': 696, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:21:05,755]\u001b[0m Trial 96 finished with value: 0.09081777466224233 and parameters: {'learning_rate': 0.03751356340259995, 'n_estimators': 683, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:21:20,354]\u001b[0m Trial 97 finished with value: 0.09097185455061034 and parameters: {'learning_rate': 0.03769613156190062, 'n_estimators': 712, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:21:24,452]\u001b[0m Trial 98 finished with value: 0.09102581050622764 and parameters: {'learning_rate': 0.052653164397513795, 'n_estimators': 677, 'max_depth': 8}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n",
      "\u001b[32m[I 2022-12-09 07:21:34,284]\u001b[0m Trial 99 finished with value: 0.09099690645156995 and parameters: {'learning_rate': 0.061317230983251585, 'n_estimators': 744, 'max_depth': 10}. Best is trial 82 with value: 0.09058143935845717.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed = 39)\n",
    "study = optuna.create_study(\n",
    "    study_name = \"cat_parameter_opt\",\n",
    "    direction = \"minimize\", # 여기선 minimize 해준다. 오차가 score이므로 작을 수록 좋다.\n",
    "    sampler = sampler)\n",
    "study.optimize(objective, n_trials = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "gpdt_A_1qduW",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1668058269063,
     "user": {
      "displayName": "신웅재",
      "userId": "10740416681947312578"
     },
     "user_tz": -540
    },
    "id": "gpdt_A_1qduW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score : 0.09058143935845717\n",
      "Best trial : {'learning_rate': 0.07525919980319846, 'n_estimators': 805, 'max_depth': 10}\n"
     ]
    }
   ],
   "source": [
    "# 내가 해봤을 때는 여기서 0.08이 아니라 그 이상값 예를 들어서 0.09가 뜨면\n",
    "# 보통 성능이 좋지 않았다.\n",
    "# 0.09 이상 뜬다면 Teacher model을 다시 만들어 볼 것 -> 시간 낭비 하지 말고\n",
    "# 참고로 가장 좋았던 모델은 0.081이 떴다.\n",
    "\n",
    "print(\"Best Score :\", study.best_value)\n",
    "print(\"Best trial :\", study.best_trial.params)\n",
    "# 8 모델 'learning_rate': 0.11237055673492984, 'n_estimators': 244, 'max_depth': 9 -> 0.08553467733779072\n",
    "# 18 모델 'learning_rate': 0.013666536260931852, 'n_estimators': 953, 'max_depth': 9 -> 0.08653234054058978\n",
    "# pca 모델 'learning_rate': 0.07525919980319846, 'n_estimators': 805, 'max_depth': 10 -> 0.09058143935845717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "372f538f-3bc6-4d46-9066-5c63d2f68e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095, 18)\n",
      "(14095,)\n",
      "(6041, 18)\n"
     ]
    }
   ],
   "source": [
    "print(X_train2.shape)\n",
    "print(y_train2.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6686f093-0c54-4e59-995d-71084031fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14095,)\n",
      "(6041,)\n",
      "training model for CV #1\n",
      "training model for CV #2\n",
      "training model for CV #3\n",
      "training model for CV #4\n",
      "training model for CV #5\n"
     ]
    }
   ],
   "source": [
    "n_fold = 5\n",
    "cv = KFold(n_splits = n_fold, shuffle = True, random_state = 39)\n",
    "\n",
    "cat_val = np.zeros((X_train2.shape[0]))\n",
    "cat_test = np.zeros((X_test.shape[0]))\n",
    "\n",
    "print(cat_val.shape)\n",
    "print(cat_test.shape)\n",
    "\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(X_train2, y_train2), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    optuna_cat = CatBoostRegressor(\n",
    "        random_state = 39,\n",
    "        learning_rate = 0.013666536260931852,    # 초모수 꼭꼭 변경해줘라\n",
    "        n_estimators = 953,                    # 초모수 꼭꼭 변경해줘라\n",
    "        max_depth = 9)                        # 초모수 꼭꼭 변경해줘라\n",
    "\n",
    "    optuna_cat.fit(X_train2.loc[i_trn, :], y_train2[i_trn], verbose = False)\n",
    "\n",
    "    cat_val[i_val] = optuna_cat.predict(X_train2.loc[i_val, :])\n",
    "    cat_test += optuna_cat.predict(X_test) / n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f29b107-bb31-49b5-91c1-3f43e72879c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>score</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.158488</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>12775</td>\n",
       "      <td>1203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.167114</td>\n",
       "      <td>1109</td>\n",
       "      <td>19</td>\n",
       "      <td>11783</td>\n",
       "      <td>1184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.184921</td>\n",
       "      <td>3298</td>\n",
       "      <td>103</td>\n",
       "      <td>9594</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.207492</td>\n",
       "      <td>5888</td>\n",
       "      <td>253</td>\n",
       "      <td>7004</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.240174</td>\n",
       "      <td>8431</td>\n",
       "      <td>430</td>\n",
       "      <td>4461</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.246140</td>\n",
       "      <td>10062</td>\n",
       "      <td>637</td>\n",
       "      <td>2830</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.253647</td>\n",
       "      <td>11162</td>\n",
       "      <td>777</td>\n",
       "      <td>1730</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.244113</td>\n",
       "      <td>11858</td>\n",
       "      <td>892</td>\n",
       "      <td>1034</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.227689</td>\n",
       "      <td>12258</td>\n",
       "      <td>967</td>\n",
       "      <td>634</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.197214</td>\n",
       "      <td>12477</td>\n",
       "      <td>1026</td>\n",
       "      <td>415</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.165944</td>\n",
       "      <td>12614</td>\n",
       "      <td>1069</td>\n",
       "      <td>278</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.127631</td>\n",
       "      <td>12716</td>\n",
       "      <td>1109</td>\n",
       "      <td>176</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.097455</td>\n",
       "      <td>12787</td>\n",
       "      <td>1136</td>\n",
       "      <td>105</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.079572</td>\n",
       "      <td>12840</td>\n",
       "      <td>1151</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.065986</td>\n",
       "      <td>12864</td>\n",
       "      <td>1161</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.054313</td>\n",
       "      <td>12877</td>\n",
       "      <td>1169</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.051406</td>\n",
       "      <td>12882</td>\n",
       "      <td>1171</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.045234</td>\n",
       "      <td>12885</td>\n",
       "      <td>1175</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.043689</td>\n",
       "      <td>12886</td>\n",
       "      <td>1176</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.040552</td>\n",
       "      <td>12887</td>\n",
       "      <td>1178</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.040552</td>\n",
       "      <td>12887</td>\n",
       "      <td>1178</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>12890</td>\n",
       "      <td>1178</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>12890</td>\n",
       "      <td>1178</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>12890</td>\n",
       "      <td>1178</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.040683</td>\n",
       "      <td>12891</td>\n",
       "      <td>1178</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039120</td>\n",
       "      <td>12892</td>\n",
       "      <td>1179</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.039120</td>\n",
       "      <td>12892</td>\n",
       "      <td>1179</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>12892</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>12892</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>12892</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>12892</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.034314</td>\n",
       "      <td>12892</td>\n",
       "      <td>1182</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.031097</td>\n",
       "      <td>12892</td>\n",
       "      <td>1184</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.029484</td>\n",
       "      <td>12892</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>12892</td>\n",
       "      <td>1187</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>12892</td>\n",
       "      <td>1187</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.026251</td>\n",
       "      <td>12892</td>\n",
       "      <td>1187</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.023007</td>\n",
       "      <td>12892</td>\n",
       "      <td>1189</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>12892</td>\n",
       "      <td>1191</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>12892</td>\n",
       "      <td>1191</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.016488</td>\n",
       "      <td>12892</td>\n",
       "      <td>1193</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>12892</td>\n",
       "      <td>1196</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>12892</td>\n",
       "      <td>1197</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>12892</td>\n",
       "      <td>1198</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>12892</td>\n",
       "      <td>1198</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>12892</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>12892</td>\n",
       "      <td>1201</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12892</td>\n",
       "      <td>1203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12892</td>\n",
       "      <td>1203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12892</td>\n",
       "      <td>1203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold     score     TP    FP     FN    TN\n",
       "0        0.00  0.158488    117     0  12775  1203\n",
       "1        0.02  0.167114   1109    19  11783  1184\n",
       "2        0.04  0.184921   3298   103   9594  1100\n",
       "3        0.06  0.207492   5888   253   7004   950\n",
       "4        0.08  0.240174   8431   430   4461   773\n",
       "5        0.10  0.246140  10062   637   2830   566\n",
       "6        0.12  0.253647  11162   777   1730   426\n",
       "7        0.14  0.244113  11858   892   1034   311\n",
       "8        0.16  0.227689  12258   967    634   236\n",
       "9        0.18  0.197214  12477  1026    415   177\n",
       "10       0.20  0.165944  12614  1069    278   134\n",
       "11       0.22  0.127631  12716  1109    176    94\n",
       "12       0.24  0.097455  12787  1136    105    67\n",
       "13       0.26  0.079572  12840  1151     52    52\n",
       "14       0.28  0.065986  12864  1161     28    42\n",
       "15       0.30  0.054313  12877  1169     15    34\n",
       "16       0.32  0.051406  12882  1171     10    32\n",
       "17       0.34  0.045234  12885  1175      7    28\n",
       "18       0.36  0.043689  12886  1176      6    27\n",
       "19       0.38  0.040552  12887  1178      5    25\n",
       "20       0.40  0.040552  12887  1178      5    25\n",
       "21       0.42  0.040650  12890  1178      2    25\n",
       "22       0.44  0.040650  12890  1178      2    25\n",
       "23       0.46  0.040650  12890  1178      2    25\n",
       "24       0.48  0.040683  12891  1178      1    25\n",
       "25       0.50  0.039120  12892  1179      0    24\n",
       "26       0.52  0.039120  12892  1179      0    24\n",
       "27       0.54  0.037520  12892  1180      0    23\n",
       "28       0.56  0.037520  12892  1180      0    23\n",
       "29       0.58  0.037520  12892  1180      0    23\n",
       "30       0.60  0.037520  12892  1180      0    23\n",
       "31       0.62  0.034314  12892  1182      0    21\n",
       "32       0.64  0.031097  12892  1184      0    19\n",
       "33       0.66  0.029484  12892  1185      0    18\n",
       "34       0.68  0.026251  12892  1187      0    16\n",
       "35       0.70  0.026251  12892  1187      0    16\n",
       "36       0.72  0.026251  12892  1187      0    16\n",
       "37       0.74  0.023007  12892  1189      0    14\n",
       "38       0.76  0.019753  12892  1191      0    12\n",
       "39       0.78  0.019753  12892  1191      0    12\n",
       "40       0.80  0.016488  12892  1193      0    10\n",
       "41       0.82  0.011570  12892  1196      0     7\n",
       "42       0.84  0.009926  12892  1197      0     6\n",
       "43       0.86  0.008278  12892  1198      0     5\n",
       "44       0.88  0.008278  12892  1198      0     5\n",
       "45       0.90  0.004975  12892  1200      0     3\n",
       "46       0.92  0.003320  12892  1201      0     2\n",
       "47       0.94  0.000000  12892  1203      0     0\n",
       "48       0.96  0.000000  12892  1203      0     0\n",
       "49       0.98  0.000000  12892  1203      0     0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 역시 참고용\n",
    "\n",
    "scores = []\n",
    "TP = []\n",
    "FP = []\n",
    "FN = []\n",
    "TN = []\n",
    "for threshold in range(50) :\n",
    "    threshold = threshold / 50\n",
    "    pred = cat_val\n",
    "    pred = np.where(pred >= threshold, 1, 0)\n",
    "    score = f1_score(y_train, pred)\n",
    "    scores.append(score)\n",
    "    TP.append(confusion_matrix(y_train, pred)[0][0])\n",
    "    FN.append(confusion_matrix(y_train, pred)[0][1])\n",
    "    FP.append(confusion_matrix(y_train, pred)[1][0])\n",
    "    TN.append(confusion_matrix(y_train, pred)[1][1])\n",
    "    \n",
    "\n",
    "temp1 = pd.DataFrame(np.linspace(0, 0.98, 50), columns = ['threshold'])\n",
    "temp2 = pd.DataFrame(scores, columns = ['score'])\n",
    "temp3 = pd.DataFrame(TP, columns = ['TP'])\n",
    "temp4 = pd.DataFrame(FP, columns = ['FP'])\n",
    "temp5 = pd.DataFrame(FN, columns = ['FN'])\n",
    "temp6 = pd.DataFrame(TN, columns = ['TN'])\n",
    "scores = pd.concat([temp1, temp2, temp3, temp4, temp5, temp6], axis = 1)\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e244681-f133-4d12-b3f3-535612e7245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.158488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.160811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.167114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.175642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.184921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.196644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.207492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.223834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.240174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.250230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.246140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.251795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.253647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.252089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.244113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.237378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.227689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.217459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.197214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.180318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.165944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.145928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.127631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.108298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.097455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.085779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.079572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.073095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.065986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.055511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.054313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.052927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.051406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.048309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.045234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.043654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.043689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.043725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.040552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.040552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.040552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.040650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.040683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.040683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.039120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold     score\n",
       "0        0.00  0.158488\n",
       "1        0.01  0.160811\n",
       "2        0.02  0.167114\n",
       "3        0.03  0.175642\n",
       "4        0.04  0.184921\n",
       "5        0.05  0.196644\n",
       "6        0.06  0.207492\n",
       "7        0.07  0.223834\n",
       "8        0.08  0.240174\n",
       "9        0.09  0.250230\n",
       "10       0.10  0.246140\n",
       "11       0.11  0.251795\n",
       "12       0.12  0.253647\n",
       "13       0.13  0.252089\n",
       "14       0.14  0.244113\n",
       "15       0.15  0.237378\n",
       "16       0.16  0.227689\n",
       "17       0.17  0.217459\n",
       "18       0.18  0.197214\n",
       "19       0.19  0.180318\n",
       "20       0.20  0.165944\n",
       "21       0.21  0.145928\n",
       "22       0.22  0.127631\n",
       "23       0.23  0.108298\n",
       "24       0.24  0.097455\n",
       "25       0.25  0.085779\n",
       "26       0.26  0.079572\n",
       "27       0.27  0.073095\n",
       "28       0.28  0.065986\n",
       "29       0.29  0.055511\n",
       "30       0.30  0.054313\n",
       "31       0.31  0.052927\n",
       "32       0.32  0.051406\n",
       "33       0.33  0.048309\n",
       "34       0.34  0.045234\n",
       "35       0.35  0.043654\n",
       "36       0.36  0.043689\n",
       "37       0.37  0.043725\n",
       "38       0.38  0.040552\n",
       "39       0.39  0.040552\n",
       "40       0.40  0.040552\n",
       "41       0.41  0.040650\n",
       "42       0.42  0.040650\n",
       "43       0.43  0.040650\n",
       "44       0.44  0.040650\n",
       "45       0.45  0.040650\n",
       "46       0.46  0.040650\n",
       "47       0.47  0.040683\n",
       "48       0.48  0.040683\n",
       "49       0.49  0.039088\n",
       "50       0.50  0.039120"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이걸 해주고 나서 score 값이 가장 클 때의 threshold 값을 찾아준다.\n",
    "# 뭐 보통 0.10 ~ 0.20 사이에 나타날 것이다.\n",
    "# 그렇다고 해서 꼭 저 사이에만 나타나는 건 아니다. -> 하지만 저 사이값이 아니라면 좋지 않을 가능성이 많다.\n",
    "\n",
    "scores = []\n",
    "for threshold in range(100) :\n",
    "    threshold = threshold / 100\n",
    "    pred = cat_val\n",
    "    pred = np.where(pred >= threshold, 1, 0)\n",
    "    score = f1_score(y_train, pred)\n",
    "    scores.append(score)\n",
    "\n",
    "temp1 = pd.DataFrame(np.linspace(0, 0.99, 100), columns = ['threshold'])\n",
    "temp2 = pd.DataFrame(scores, columns = ['score'])\n",
    "scores = pd.concat([temp1, temp2], axis = 1)\n",
    "scores.loc[: 50, :]\n",
    "\n",
    "# 8 모델 -> 0.256393"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bcc4a31-872a-42ee-82db-f0c6a4078bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.253647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold     score\n",
       "12       0.12  0.253647"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.loc[scores['score'] == scores['score'].max(), :]\n",
    "\n",
    "# 최고 모델 -> 0.262664\n",
    "# 18 모델 -> 0.256393\n",
    "# 8 모델 -> 0.253647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "563c9ed2-5aa7-4d8c-85a0-4b2f2e6b51eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 5523, 1.0: 518})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그리고 앞에서 구한 threshold 값을 네번째 줄인\n",
    "# if cat_test[i] >= [여기]\n",
    "# [여기] 부분에 넣어줘 본다.\n",
    "# 만약 Counter(answer)을 보았을 때, 1.0 : ???\n",
    "# 저 물음표 값이 500에 가깝지 않다면 [여기] 부분에서 0.01을 더하든 빼든 계속 조정해주면서\n",
    "# 500 가까이 되도록 맞추는 것이 좋을 것이다.\n",
    "# 궁금하면 이 작업 해주지 말고 그냥 [여기]로 해보면 된다.\n",
    "# 당신이 맞을 수도 있다.\n",
    "# 하지만 지금까지 해본거로는 500대가 아니면 성능이 안좋았다.\n",
    "\n",
    "answer = np.zeros(cat_test.shape[0])\n",
    "\n",
    "for i in range(cat_test.shape[0]) :\n",
    "  if cat_test[i] >= 0.12 :\n",
    "    answer[i] = 1\n",
    "\n",
    "Counter(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dac8eb4-d8e1-4641-aaeb-8e7afc0567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출하면 끝이다.\n",
    "\n",
    "submission_preds = answer\n",
    "submission = pd.read_csv('/home/studio-lab-user/MYDATA/Construction Machine Oil/open/sample_submission.csv')\n",
    "submission['Y_LABEL'] = submission_preds\n",
    "submission.to_csv('/home/studio-lab-user/MYDATA/Construction Machine Oil/결과/submission_var_10_8_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd7893-ff8c-4253-823e-deb466d22ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9735eb32-fb09-4104-9e1d-78db26f93039",
   "metadata": {},
   "source": [
    "모델 선택 전까지 해볼 것\n",
    "\n",
    "1. SMOTE 다른 변수 모두 있을 때 해보기 X\n",
    "2. ADASYN X\n",
    "3. MISSFOREST 써서 결측값 채우고 해보기 △  \n",
    "-> 일단 더 생각해 봐야 겠지만, 솔직히 통계학적으로 보았을 때는 결측 변수는 너무 많이 결측되면 버리는게 맞음\n",
    "4. 차원 축소 -> PACMAP 등 -> 나름 써볼 수 있을 듯\n",
    "5. Student 모델 변수 줄이기 -> 모델 선택 되고 나서\n",
    "6. GBM 사용해보기 X\n",
    "7. IsorationForest X\n",
    "8. 표준화 min-max 사용해보기                 -> 모델 선택 되고 나서\n",
    "9. DBSCAN, HDBSCAN -> 별로\n",
    "10. TABNET -> 이건 애들보고 해보라고 하기\n",
    "11. PermutationImportance 로 변수 찾기       -> 모델 선택 되고 나서"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6f427-ea34-4ab0-9738-a0284d41af36",
   "metadata": {},
   "source": [
    "Catboost로 선택되고 나서 할 것\n",
    "\n",
    "1. Teacher, Student 모델 변수 줄이기 -> 매우 중요\n",
    "2. 표준화 사용해보기 -> 아마 큰 영향을 없을 듯 -> Tree 모형이므로\n",
    "3. PermutationImportance 로 변수 찾기\n",
    "4. 특히 Teacher 모델을 적합시키고 나서 Student 모델의 MAE가 0.8로 떨어져야 함 -> 그래야 좋은 모형이 적합됨  \n",
    "하지만 대부분 0.9 이상이 나옴\n",
    "5. 통계 방법 적용시키기\n",
    "6. Stacking Ensemble\n",
    "7. 분류 지표(ROC_AUC), 회귀 지표(MAE) 바꾸기\n",
    "8. 로지스틱 분포로 유의한 변수 찾기\n",
    "9. 과연 test 데이터의 AL를 만들어낼 수 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf493ca-a2e9-47fa-a29c-1b72c2fcb48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e84158-5930-42c9-a75b-fab5876a19ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d396f65-8636-4b45-b995-7ebc5c2f677d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68f7d7-0122-4a8c-b72a-a11ef241da96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c1307-6707-43fd-bafb-f3c763936941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b43dcb-5a7a-4ac8-a031-e0c0c414f60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15267131-167a-451d-8bb2-5077719daf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1074e92-d543-4c12-9ef4-97c26a9e74d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43b93f-3357-42c0-b027-9ba54e696b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cbe5a8-5157-46e1-a778-63ba8c100cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f2152-2c8c-46d4-8ee0-64ccb6f6d6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25158db1-d96d-4a69-b527-6c5f3aafb557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cef174-b16c-4c86-af9b-81256fa7aaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
